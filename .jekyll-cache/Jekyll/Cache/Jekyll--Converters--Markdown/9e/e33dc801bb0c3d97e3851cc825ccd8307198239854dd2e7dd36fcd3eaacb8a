I"Lâ<p>You can find below the list of past seminars.</p>

<p>
  January 19th, 2021, <a href="https://cpaf.cnrs.fr/spip.php?article27">Daniel Stoekl</a> (√âcole Pratique des Hautes √âtudes)
  
  
  
  
  <br />
  <b>Title:</b> <i>De la transcription automatique de manuscrits h√©breux m√©di√©vaux via l'√©dition scientifique √† l'analyse de l'intertextualit√© : outils et praxis autour d'eScriptorium</i><br />
  <b>Abstract:</b> Following a brief introduction to our open-source HTR infrastructure <a href="https://gitlab.inria.fr/scripta/escriptorium/">eScriptorium</a> cum <a href="https://github.com/mittagessen/kraken">kraken</a> I will demonstrate its application to the automatic layout segmentation, handwritten textsegmentation and paleography of Hebrew manuscripts. Using its rich (but still growing) internal functionalities and API as well as a number of external tools (Decker et alii 2011, Shmidman et alii 2018 and my own), I will deal with automatic text identification, alignment and <a href="https://tikkoun-sofrim.firebaseapp.com/">crowdsourcing</a> (Kuflik et al 2019, Wecker et al 2019) and how these procedures can be used to create different types of generic models for segmentation and transcription. I will show first ideas for automatically passing from a document hierarchy resulting from HTR to a text oriented model with integrated interlinear and marginal additions that can be displayed in tools like <a href="https://editions.erabbinica.org/S07326.xml">TEI-Publisher</a>. While the methods presented are generic and applicable to most languages and scripts, special attention will be given to problems evolving from dealing with non-Latin scripts, RTL and morphologically rich languages.  <br /> <i>Bibliography:</i><br /> - Dekker, R. H., Middell, G.: Computer-Supported Collation with CollateX: Managing Textual Variance in an Environment with Varying Requirements. Supporting Digital Humanities 2011. University of Copenhagen, Denmark (2011).  <br /> - Kuflik, T. M. Lavee, A. Ohali, V. Raziel-Kretzmer, U. Schor, A. Wecker, E. Lolli, P. Signoret, D. St√∂kl Ben Ezra (2019) 'Tikkoun Sofrim ‚Äì Combining HTR and Crowdsourcing for Automated Transcription of Hebrew Medieval Manuscripts', DH2019.  <br /> - Lapin, Hayim and Daniel St√∂kl Ben Ezra, <a href="https://editions.erabbinica.org/">eRabbinica</a> <br /> - Meier, Wolfgang, Magdalena Turska, TEI Processing Model Toolbox: Power To The Editor. DH 2016: 936  <br /> - Meier, Wolfgang, Turska, Magdalena, <a href="https://gitlab.existsolutions.com/tei-publisher">TEI-Publisher</a>.  <br /> - Shmidman, A., Koppel, M., Porat, E.: Identification of parallel passages across a large hebrew/aramaic corpus. Journal of Data Mining and Digital Humanities, 2018  <br /> - Wecker, A. V. Raziel-Kretzmer, U. Schor, T. Kuflik, A. Ohali, D. Elovits, M. Lavee, P. Stevenson, D. St√∂kl Ben Ezra, (2019) 'Tikkoun Sofrim: A WebApp for Personalization and Adaptation of Crowdsourcing Transcriptions', UMAP‚Äô19 Adjunct (Larnaca. New York: ACM Press)<br />
  </p>

<p>
  December 15th, 2020, <a href="http://www.numapresse.org/annuaire/pierre-carl-langlais/">Pierre-Carl Langlais</a> (Paris-IV Sorbonne)
  
  
  
  
  <br />
  <b>Title:</b> <i>Redefining the cultural history of newspapers with artificial intelligence: the experiments of the Numapresse project</i><br />
  <b>Abstract:</b> During the last twenty years, libraries developed massive digitization program. While this shift has significantly enhanced the accessibility cultural digital archives, it has also opened up unprecedented research opportunities. Innovative projects have recently attempted to apply large scale quantitative methods borrowed from computer science to tackle ambitious historical issues. The Numapresse project proposes a new cultural history of French newspaper from 1800, notably through the distant reading of detailed digitization outputs from the French National Library and other partners. It has recently become a pilot project of the future data labs of the French National Library. This presentation features a series of 'operationalization' of core concepts of the cultural history of the news in the context of a continuous methodological dialog with statistics, data science, and machine learning. Classic methods of text mining have been supplemented with spatial analysis of pages to deal with the complex and polyphonic editorial structures of newspapers in order to retrieve specific formats like signatures or news dispatch. The project has created a library of 'genre models' which made it possible to retrieve large collections of texts belong to leading newspaper genres in different historical settings. This approach has been extended to large collections of newspaper images through the retraining of deep learning models. The automated identification of text and image reprints also makes it possible to map the transforming ecosystem of French networks and its connection to other publication formats. The experimental work of Numapresse aims to foster a modeling ecosystem among research and library communities working on cultural heritage archives.
  </p>

<p>
  November 24, 2020, <a href="http://igm.univ-mlv.fr/~gambette/">Philippe Gambette</a> (Universit√© Paris-Est Marne-la-Vall√©e)
  
  
  
  
  <br />
  <b>Title:</b> <i>Alignment and text comparison for digital humanities</i><br />
  <b>Abstract:</b> This talk will provide several algorithmic approaches based on alignment or text comparison algorithms, at different scales, with applications in digital humanities. We will present an alignment-based approach for 16th and 17th century French text modernisation and show the impact of this normalisation process on automatic geographical named entity recognition.<br /> We will also show several visualisation techniques which are useful to explore text corpora by highlighting similarities and differences between those texts at different levels. In particular, we will illustrate the use of Sankey diagrams at different levels to align various editions of the same text, such as poetry books by Marceline Desbordes-Valmore published from 1819 to 1830 or Heptameron by Marguerite de Navarre. This visualisation tool can also be used to contrast the most frequent words of two comparable corpora to highlight their differences. We will also illustrate how the use of word trees, built with the TreeCloud software, helps identifying trends in a corpus, by comparing the trees built for subsets of the corpus.<br /> We will finally focus on stemmatology, where the analysed texts are supposed to be derived from a unique initial manuscript. We will describe a tree reconstruction algorithm designed to take linguistic input into account when building a tree describing the history of the manuscripts, as well as a list of observed variants supporting its edges.<br /> Contributors of these works include Delphine Amstutz, Jean-Charles Bontemps, Aleksandra Chaschina, Hilde Eggermont, Rapha√´l Gaudy, Eleni Kogkitsidou, Gregory Kucherov, Tita Kyriacopoulou, Nad√®ge Lechevrel, Xavier Le Roux, Claude Martineau, William Martinez, Anna-Livia Morand, Jonathan Poinhos, Caroline Trotot and Jean V√©ronis.
  </p>

<p>
  October 20, 2020, <a href="https://dhai-seminar.github.io/">The DHAI team</a> (DHAI)
  
  
  
  
  <br />
  <b>Title:</b> <i>Heads and Tails: When Digital Humanities and Artificial Intelligence Meet.</i><br />
  <b>Abstract:</b> Joint presentation of the DHAI team, which serves as an introduction to this second season of DHAI.
  </p>

<p>
  October 9, 2020, <a href="https://apps.uqo.ca/DosEtuCorpsProf/PageProfesseur.aspx?id=karine.gentelet@uqo.ca">Karine Gentelet</a> (Universit√© du Qu√©bec en Outaouais (Canada))
  
    [<a href="https://savoirs.ens.fr/expose.php?id=3897">video</a>]
  
  
  
  <br />
  <b>Title:</b> <i>Reflections on the decolonization processes and data sovereignty based on the digital and AI strategies of indigenous peoples in Canada</i><br />
  <b>Abstract:</b> This presentation will focus on the digital strategies developed by Indigenous Peoples to reaffirm their information sovereignty and how they contribute to the decolonization of data. The information that represents Indigenous Peoples is tainted by colonization and systemic practices of informational discrimination. Their initiatives of informational sovereignty and data decolonization allows data that is collected by and for them and therefore much more accurate, diversified and representative of their realities and needs. The principles developed by Indigenous Peoples not only testify to an asserted digital agency but also induce a paradigmatical shift due to the inclusion of ancestral knowledge and traditional modes of governance. It allows a new power balance within the digital ecosystem. (see below for the French version of this abstract)
  </p>

<p>
  October 9, 2020, <a href="https://apps.uqo.ca/DosEtuCorpsProf/PageProfesseur.aspx?id=karine.gentelet@uqo.ca">Karine Gentelet</a> (Universit√© du Qu√©bec en Outaouais (Canada))
  
    [<a href="https://savoirs.ens.fr/expose.php?id=3897">video</a>]
  
  
  
  <br />
  <b>Title:</b> <i>R√©flexions sur les processus de d√©colonisation et souverainet√© des donn√©es √† partir des strat√©gies num√©riques et d‚ÄôIA des Peuples autochtones au Canada</i><br />
  <b>Abstract:</b> Cette pr√©sentation portera sur les strat√©gies num√©riques d√©velopp√©es par les Peuples autochtones pour r√©affirmer leur souverainet√© en mati√®re d'information et sur la mani√®re dont ils contribuent √† la d√©colonisation des donn√©es. Les informations qui repr√©sentent les Peuples autochtones sont biais√©es du fait de la colonisation et des pratiques syst√©miques de discrimination informationnelle. Leurs initiatives de souverainet√© informationnelle et de d√©colonisation des donn√©es permettent de recueillir des donn√©es par et pour eux et donc beaucoup plus pr√©cises, diversifi√©es et repr√©sentatives de leurs r√©alit√©s et de leurs besoins. Les principes d√©velopp√©s par les Peuples autochtones t√©moignent non seulement d'une agence num√©rique affirm√©e, mais induisent √©galement un changement de paradigme d√ª √† l'inclusion des connaissances ancestrales et des modes de gouvernance traditionnels. Ils permettent un nouvel √©quilibre des pouvoirs au sein de l'√©cosyst√®me num√©rique.
  </p>

<p>
  June 8, 2020, <a href="https://www.casilli.fr">Antonio Casilli</a> (Paris School of Telecommunications (Telecom Paris))
  
  
  
  
  <br />
  <b>Title:</b> <i>The last mile of inequality: What COVID-19 is doing to labor and automation</i><br />
  <b>Abstract:</b> The ongoing COVID-19 crisis, with it lockdowns, mass unemployment, and increased health risks, has been described as a automation-forcing event, poised to accelerate the introduction of automated processes replacing human workers. Nevertheless, a growing body of literature has emphasized the human contribution to machine learning. Especially platform-based digital labor performed by global crowds of underpaid micro-workers or extracting data from cab-hailing drivers and bike couriers, turns out to play an crucial role. Although the pandemic has been regarded as the triumph of 'smart work', telecommuting during periods of lockdown and closures concern only about 25 percents of workers. A class gradient seems to be at play, as platform-assisted telework is common among higher-income brackets, while people on lower rungs of the income ladder are more likely to hold jobs that involve physical proximity, which are deemed essential and cannot be moved online or interrupted. These include two groups of contingent workers performing what can be described as 'the last mile of logistics' (delivery, driving, maintenance and other gigs at the end of the supply chain) and the 'last mile of automation' (human-in-the-loop tasks such as data preparation, content moderation and algorithm verification). Indeed during lockdown, both logistic and micro-work platforms have reported a rise in activity ‚Äì with millions signing up to be couriers, drivers, moderators, data trainers. The COVID-19 pandemic has thus given unprecedented visibility to these workers, but without increased social security. Their activities are equally carried out in public spaces, in offices, or from home‚Äîyet they generally expose workers to higher health risks with poor pay, no insurance, and no sick leave. Last mile platform workers shoulder a disproportionate share of the risk associated with ensuring economic continuity. Emerging scenarios include use of industrial actions to increase recognition and improve their working conditions. COVID-19 has opened spaces of visibility by organizing workers across Europe, South America, and the US. Since March 2020, Instacart walkouts, Glovo and Deliveroo street rallies, Amazon 'virtual walkouts' have started demanding health measures or protesting remuneration cuts.
  </p>

<p>
  June 4, 2020, <a href="https://www.ens.fr">Sietske Fransen &amp; Leonardo Impett</a> (Max-Planck-Institut f√ºr Kunstgeschichte)
  
  
  
  
  <br />
  <b>Title:</b> <i>Print, Code, Data: New Media Disruptions and Scientific Visualization</i><br />
  <b>Abstract:</b> This paper discusses changes in scientific diagramming in response to new media disruptions: the printing press, and online data/research code. In the first case, the role of handwritten documents and the visual forms of scientific diagramming re-align in response to the circulational economics and medial accessibility of the printing press in early modern Europe. In the second, published research code unsettles the principle, common in the second half of the twentieth century, that a peer-reviewed article in computer science ought to outline its methods with enough detail to enable repeatability.<br /> The printing press brought benefits as well as restrictions to the inclusion of diagrams in scientific works. Some of the downsides were that not every printer was able to manufacture separate wood blocks and/or copper plates that could contain the diagram as if hand-drawn. Instead, diagrams were often made entirely out of typeface. On the other hand, the quick spread of the use of printed books in addition to manuscripts, opened new roles for the manuscript as a medium of creativity. In the early days of print, it is therefore in manuscripts that we can find the visualization of scientific processes, which form the background to printed material. <br /> The information sufficient for 'replicability' in computer science (which in the physical sciences has meant 'formal experimental methodology', but in computer science is epistemically closer to the research itself) had most often been included in tables, schematizations and heavily-labelled diagrams, sometimes augmented by so-called 'pseudocode' (a kind of software caricature, which cannot itself be run on a machine). The inclusion of research code thus dramatically displaces the role of scientific diagrams in machine learning research: from a notational system which ideally contains sufficient information to reproduce an algorithm (akin to electrical circuit diagrams) to a didactic visualization technique (as in schoolbook diagrams of the Carbon Cycle). In Badiou's (1968) terminology, diagrams shift from symbolic formal systems to synthetic spatializations of non-spatial processes. The relationship between 'research output' (as the commodity produced by computer-science research groups) and its constituent components (text, diagram, code, data) is further destabilized by deep learning techniques (which rely on vast amounts of training data) : no longer are algorithms published on their own, but rather trained models, assemblages of both data and software, again shifting the onus of reproducibility (and, therefore, the function of scientific notation). The changed epistemological role of neural network visualizations allows for a far greater formal instability, leading to the rich ecology of visual solutions (Alexnet, VGG, DeepFace) to the problem of notating multidimensional neural network architectures.<br /> By comparing the impact of new media on the use, form and distribution of diagrams in the early modern period, with the impact of code on the role of diagrams in computer science publications, we are opening up a conversation about the influence of new media on science, both in history and in current practice.
  </p>

<p>
  March 30, 2020, <a href="https://www.ens.fr">Aaron Hershkowitz</a> (Institute for Advanced Study)
  
  
  
  
  <br />
  <b>Title:</b> <i>The Cutting Edge of Epigraphy: Applying AI to the Identification of Stonecutters</i><br />
  <b>Abstract:</b> Inscriptions are a vital category of evidence about the ancient world, providing a wealth of information about subject matters and geographical regions outside of the scope of surviving literary texts. However, to be most useful inscriptions need to be situated within a chronological context: the more precise the better. This kind of chronological information can sometimes be gleaned from dating formulae or events mentioned in the inscribed texts, but very often no such guideposts survive. In these cases, epigraphers can attempt to date a given text on a comparative basis with other, firmly-dated inscriptions. This comparative dating can be done on the basis of socio-linguistic patterns or the physical shape of letter forms present in the inscription. In the latter case, a very general date can be achieved on the basis of the changing popularity of particular letter forms and shapes in a particular geographic context, or a more specific date can be achieved if the 'handwriting' of a stonecutter can be identified. Such a stonecutter would have a delimited length of activity, so that if any of his inscriptions have a firm date, a range of about thirty years or less can be provided to all other inscriptions made by him. Unfortunately, very few scholars have specialized in the ability to detect stonecutter handwriting, but as was showed by an early attempt (see Panagopoulos, Papaodysseus, Rousopoulos, Dafi, and Tracy 2009, Automatic Writer Identification of Ancient Greek Inscriptions) computer vision analysis has significant promise in this area. The Krateros Project to digitize the epigraphic squeezes of the Institute for Advanced Study is actively working to pursue this line of inquiry, recognizing it as critical for the future of epigraphy generally.
  </p>

<p>
  March 2, 2020, <a href="https://www.ens.fr">Matteo Valleriani</a> (Technische Universit√§t, Berlin)
  
  
  
  
  <br />
  <b>Title:</b> <i>The Sphere. Knowledge System Evolution and the Shared Scientific Identity of Europe </i><br />
  <b>Abstract:</b> On the basis of the corpus of all early modern printed editions of commentaries on the Sphere of Sacrobosco, the lecture shows how to reconstruct the transformation process‚Äîand its mechanisms‚Äîundergone by the treatise, and so to explore the evolutionary path, between the fifteenth and the seventeenth centuries, of the scientific system pivoted around cosmological knowledge: the shared scientific identity of Europe. The sources are analyzed on three levels: text, images, and tables. From a methodological point of view the lecture will also show how data are extracted by means of machine learning and analyzed by means of an approach derived from the physics of the complex systems and network theory.
  </p>

<p>
  February 3, 2020, <a href="http://production-scientifique.bnf.fr/CV/bermes-emmanuelle">Emmanuelle Berm√®s and Jean-Philippe Moreux</a> (BnF)
  
  
  
  
  <br />
  <b>Title:</b> <i>From experimentation to community building: AI at the BnF</i><br />
  <b>Abstract:</b> Artificial intelligence has been present at the BnF for more than 10 years, at least in its 'machine learning' version, through R&amp;D projects conducted with the image and document analysis community. But we can imagine that the rise and fall of expert systems at the beginning of the 1990s will also have questioned the BnF, as our American colleagues did: 'Artificial Intelligence and Expert Systems: Will They Change the Library?' (Linda C. Smith, F. W. Lancaster, University of Illinois, 1992). <br />Today, the democratization of deep learning promotes the ability to experiment and carry out in virtual autonomy, but also and above all makes possible interdisciplinary projects where expertise on content, data and processing is required. This conference will be an opportunity to present the results of such a project, dedicated to the visual indexing of Gallica's iconographic content, to share our feedback and to consider a common dynamic driven by the needs and achievements of the field of digital humanities practice. <br /> The presentation will place these experiments in the BnF's overall strategy for services to the researchers, but will also broaden the scope by addressing the overall positioning of libraries with regard to AI.
  </p>

<p>
  January 6, 2020, <a href="http://www.chartes.psl.eu/fr/jean-baptiste-camps">Jean-Baptiste Camps</a> (ENC)
  
  
  
  
  <br />
  <b>Title:</b> <i>Philology, old texts and machine learning</i><br />
  <b>Abstract:</b> Phrase de pr√©sentation: I will give an introduction to machine learning techniques applied to old documents (manuscripts) and texts, ranging from text acquisition (e.g. handwritten text recognition) to computational data analysis (e.g. authorship attribution).
  </p>

<p>
  January 6, 2020, <a href="https://www.ens.fr">Alexandre Guilbaud and Stavros Lazaris</a> (Universit√© Pierre et Marie Curie / CNRS)
  
  
  
  
  <br />
  <b>Title:</b> <i>La circulation de l‚Äôillustration scientifique au Moyen-√Çge et √† l‚Äô√©poque moderne</i><br />
  <b>Abstract:</b> Nous vivons entour√©s d‚Äôimages. Elles nous portent, nous charment ou nous d√©√ßoivent et cela √©tait √©galement le cas, √† des degr√©s diff√©rents bien entendu, pour l‚Äôhomme durant le Moyen Age et l‚Äô√©poque moderne. Comment les images ont-elles fa√ßonn√© sa pens√©e dans le domaine des sciences et dans quelles mesures en sont-elles repr√©sentatives ? Quelle √©tait la nature des illustrations scientifiques et comment les acteurs de ces √©poques les ont-t-il mises au point et utilis√©es ? Les p√©riodes m√©di√©vale et moderne sont particuli√®rement propices pour mener une recherche sur la constitution d‚Äôune pens√©e visuelle li√©e aux savoirs scientifiques. Cet expos√© sera l'occasion de pr√©senter un projet de recherche en Humanit√©s num√©riques visant √† contribuer √† cette probl√©matique en examinant de quelle fa√ßon les d√©veloppements actuels dans les domaines de l‚ÄôIA et de la vision artificielle permettent d‚Äôenvisager des approches nouvelles pour l‚Äôanalyse historique de la circulation de l‚Äôillustration scientifique au cours de ces deux p√©riodes. Nous pr√©senterons √† cette occasion les corpus s√©lectionn√©s pour cette √©tude (les manuscrits contenant le Physiologus et le De Materia medica de Dioscoride pour le Moyen Age ; les planches d‚Äôhistoire naturelle et sciences math√©matiques  dans le corpus des dictionnaires et encyclop√©dies au XVIIIe si√®cle) et montrerons, sur des exemples, comment les modes de circulation qui sont √† l‚Äô≈ìuvre dans ces corpus appellent notamment le d√©veloppement de nouvelles techniques, bas√©es sur la reconnaissance des formes et la mise en relation entre textes et images.
  </p>

<p>
  January 6, 2020, <a href="https://www.ens.fr">V√©ronique Burnod</a> (Conservateur en chef des mus√©es de France)
  
  
  
  
  <br />
  <b>Title:</b> <i>Comment les historiens d'art peuvent-ils contribuer au deep learning?</i><br />
  <b>Abstract:</b> Certaines oeuvres d'art doivent √™tre identifi√©es en l'absence d'archives, voire m√™me au travers de repeints, ce qui complique la donne. L'√©tude des 'points informatifs' nous renseigne sur l'artiste et sur notre compr√©hension de ce dernier. Elle nous apporte un √©clairage in√©dit lorsque l'oeuvre n'est pas lisible (mauvaises restaurations, par exemple). Cette nouvelle m√©thode d'analyse donne d'excellents r√©sultats comme le d√©montreront les d√©couvertes d'une √©tude de Michel Ange pour le Jugement dernier de la Sixtine (acquise ensuite par le Louvre) et de 'la Dormeuse de Naples' d'Ingres, une oeuvre mythique disparue parmi les plus recherch√©es au monde. Cette expertise autorise d√©sormais les historiens d'art qui sont form√©s √† cette discipline √† structurer les banques d'images des mus√©es de France. Des travaux d'√©tudiants r√©alis√©s √† Lille 3 en donnent la preuve. D√©sormais l'objectif est de r√¥der les syst√®mes en IA sur les banques d'images des Mus√©es de France en lien avec le service de Jean Ponce. Mais comment avancer tant que cette discipline scientifique ne sera pas reconnue chez les Historiens d'Art aux plans national et international ? Actuellement cela paralyse les travaux possibles en lien avec l'IA, l'envergure et l'incidence du projet n√©cessitant une v√©ritable expertise dans ce domaine.
  </p>

<p>
  December 2, 2019, <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a> (ENPC)
  
  
  
  
  <br />
  <b>Title:</b> <i>Machine learning and text analysis for digital humanities</i><br />
  <b>Abstract:</b> I will present key concepts and challenges of Deep Learning approaches and in particular their applications on images for digital humanities. The presentation will use three concrete examples to introduce these concepts and challenges: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3347175">artwork price prediction</a>, <a href="http://imagine.enpc.fr/~shenx/Watermark/">historical watermark recognition</a>, and <a href="http://imagine.enpc.fr/~shenx/ArtMiner/">pattern recognition and discovery in artwork datasets</a>.
  </p>

<p>
  November 5, 2019, <a href="http://www.lattice.cnrs.fr/membres/direction/thierry-poibeau/">Thierry Poibeau, Mathilde Roussel and Matthieu Raffard, Tim Van De Cruys</a> (Lattice (CNRS)/ IRIT (Toulouse))
  
    [<a href="">video</a>]
  
  
    [<a href="../slides/">Slides</a>]
  
  <br />
  <b>Title:</b> <i>Oupoco, l‚Äôouvroir de po√©sie potentielle (Thierry Poibeau)</i><br />
  <b>Abstract:</b> Oupoco, l‚Äôouvroir de po√©sie potentielle. Thierry Poibeau, Lattice (Paris). La pr√©sentation portera sur le projet Oupoco, qui est largement inspir√© de l‚Äôouvrage de Raymond Queneau ¬´ Cent mille milliards de po√®mes ¬ª, paru en 1961. Dans cet ouvrage, Queneau propose 10 sonnets dont tous les vers riment, ce qui permet de les combiner librement pour composer des po√®mes respectant la forme du sonnet. Dans le cadre d‚ÄôOupoco, les po√®mes de Queneau ont √©t√© remplac√©s par des sonnets du 19e si√®cle, qui sont √† la fois libres de droit et plus vari√©s quant √† leur forme et leur structure. Un module d‚Äôanalyse (structure globale, type de rimes, etc.) a √©t√© mis en place et les informations ainsi obtenues servent de base au g√©n√©rateur produisant des sonnets respectant les r√®gles propres √† ce genre. Au-del√† de l‚Äôaspect ludique du projet, celui-ci pose des questions quant au statut de l‚Äôauteur, et quant √† la coh√©rence et la pertinence des po√®mes produits. Il suscite aussi la curiosit√©, et am√®ne par exemple souvent le lecteur √† revenir aux sonnets source pour v√©rifier quel est le sens original d'un vers donn√©. Finalement certaines extensions r√©centes du projet seront pr√©sent√©s, comme la ¬´ bo√Æte √† po√©sie ¬ª, une version portative du g√©n√©rateur Oupoco.<br /> <b>Title:</b> Pr√©sentation de la Bo√Æte √† po√©sie (Mathilde Roussel and Matthieu Raffard)<br /> <b>Abstract:</b> Bo√Æte √† po√©sie, un g√©n√©rateur de po√©sie portable et basse consommation, d√©velopp√© dans le cadre du projet Oupoco suite √† une collaboration avec <a href="http://www.raffard-roussel.com">l‚ÄôAtelier Raffard-Roussel</a>.<br /> <b>Title:</b> La g√©n√©ration automatique de po√©sie √† l'aide de r√©seaux de neurones (Tim Van De Cruys)<br /> <b>Abstract:</b> La g√©n√©ration automatique de po√©sie est une t√¢che ardue pour un syst√®me informatique. Pour qu'un po√®me ait du sens, il est important de prendre en compte √† la fois des aspects linguistiques et litt√©raires. Les mod√®les de langue bas√©s sur les r√©seaux de neurones ont am√©lior√© l'√©tat de l'art par rapport √† la mod√©lisation pr√©dictive de langage, mais quand ils sont entra√Æn√©s sur des corpus de texte g√©n√©raux, ils ne g√©n√®rent √©videmment pas de po√©sie en soi. Dans cette pr√©sentation, on explorera comment ces mod√®les - entra√Æn√©s sur des textes g√©n√©raux - peuvent √™tre adapt√©s afin de mod√©liser les aspects linguistiques et litt√©raires n√©cessaires pour la g√©n√©ration de po√©sie. Le cadre pr√©sent√© est appliqu√© √† la g√©n√©ration de po√®mes en fran√ßais, et √©valu√© √† l'aide d'une √©valuation humaine. Le projet Oupoco est soutenu par <a href="http://transfers.ens.fr">le labex Transfers</a> et <a href="https://www.psl.eu/translitterae">l‚ÄôEUR Translitterae</a>.<br /><div align="center"><img src="img/poibeau.jpg" width="400" /> </div>
  </p>

<p>
  October 7, 2019, <a href="https://www.ens.fr">L√©a Saint-Raymond / B√©atrice Joyeux-Prunel for the DHAI organizing members</a> (ENS)
  
  
  
    [<a href="../slides/2019-09-07-Saint-Raymond_introduction.pdf">Slides</a>]
  
  <br />
  <b>Title:</b> <i>When Digital Humanities meet Artificial Intelligence, an Introduction</i><br />
  <b>Abstract:</b> Introductory and methodological session on the themes of the seminars
  </p>

<p>
  September 17, 2019, <a href="https://people.eecs.berkeley.edu/~efros/">Alexei Efros</a> (UC Berkeley)
  
  
  
  
  <br />
  <b>Title:</b> <i>Finding Visual Patterns in Large Photo Collections for Visualization, Analytics, and Artistic Expression</i><br />
  <b>Abstract:</b> Our world is drowning in a data deluge, and much of this data is visual. Humanity has captured over one trillion photographs last year alone.  500 hours of video is being uploaded to YouTube every minute.   In fact, there is so much visual data out there already that much of it might never be seen by a human being! But unlike other types of 'Big Data', such as text, much of the visual content cannot be easily indexed or searched, making it Internet‚Äôs 'digital dark matter' [Perona 2010].  In this talk, I will first discuss some of the unique challenges that make visual data difficult compared to other types of content.   I will them present some of our work on navigating, visualizing, and mining for visual patterns in large-scale image collections.  Example data sources will include user-contributed Flickr photographs, Google StreetView imagery of entire cities, a hundred years of high school student portraits, and  a collection of paintings attributed to Jan Brueghel.    I will also show how recent progress in using deep learning as a way to find visual patterns and correlations could be used to synthesize novel visual content using 'image-to-image translation' paradigm.  I will conclude with examples of contemporary artists using our work as a new tool for artistic visual expression.
  </p>

<p>
  October 22, 2019, <a href="https://sites.google.com/view/emilylspratt/home">Emily L. Spratt</a> (Columbia)
  
  
  
  
  <br />
  <b>Title:</b> <i>Art, Ethics, and AI: Problems in the Hermeneutics of the Digital Image</i><br />
  <b>Abstract:</b> In the last five years, the nature of historical inquiry has undergone a radical transformation as the use of AI-enhanced search engines has become the predominant mode of knowledge investigation, consequentially affecting our engagement with images. In this system, the discovery of responses to our every question is facilitated as the vast stores of digital information that we have come to call the data universe are conjured to deliver answers that are commensurate with our human scale of comprehension, yet often exceed it. In this digital interaction it is often assumed that queries are met with complete and reliable answers, and that data is synonymous with empirical validity, despite the frequently changing structure of this mostly unsupervised repository of digital information, which in actuality projects a distortion of the physical world it represents. In this presentation, the role of vision technology and AI in navigating, analyzing, organizing, and constructing our art and art historical archives of images will be examined as a shaping force on our interpretation of the past and projection of the future. Drawing upon the observations made by Michel Foucault in The Archaeology of Knowledge that the trends toward continuity and discontinuity in descriptions of historical narratives and philosophy, respectively, are reflections of larger hermeneutic structures that in and of themselves influence knowledge formation, the question of the role of image-related data science in our humanistic interpretation of the world will be explored. Through the examples of preservationists and artists using machine learning techniques to curate and create visual information, and in consideration of the information management needs of cultural institutions, the machine-learned image will be posited as a new and radical phenomenon of our society that is altering the nature of historical interpretation itself. By extension, this presentation brings renewed attention to aesthetic theory and calls for a new philosophical paradigm of visual perception to be employed for the analysis and management of our visual culture and heritage in the age of AI, one which incorporates and actively partakes in the development of computer vision-based technologies. 
  </p>

<p>
  October 22, 2019, <a href="https://sites.google.com/view/emilylspratt/home">Emily L. Spratt</a> (Columbia)
  
  
  
  
  <br />
  <b>Title:</b> <i>Exhibition Film Screening of " Au-del√† du Terroir, Beyond AI Art," and Discussion with Curator Emily L. Spratt</i><br />
  <b>Abstract:</b>  "Au-del√† du Terroir, Beyond AI Art" is the art exhibition for the Global Forum on AI for Humanity, which is being hosted by President Macron and is sponsored by the Government of France; it is being held at the at the Institute de France, Quai Conti, Paris, October 28-30, 2019. The forum is a direct outcome of the last G7 meeting regarding the responsible use of AI. This exhibition shapes the visual tenor of the meeting and features the brilliance of some of the leading artists and cultural heritage specialists of our time who are working with AI: Hito Steyerl, Mario Klingemann, Refik Anadol, Robbie Barrat, AICAN (Ahmed Elgammal), ICONEM, and even one project in collaboration with the inimitable Chef Alain Passard. In this limited screening of the exhibition, which takes places in the form of a digital projection, curator Emily L. Spratt will discuss the issues involved with "AI art" and the implications of the creative applications of machine learning for images and videos. A recent Columbia article was <a href="https://datascience.columbia.edu/shaping-art-ai-emily-spratt-curates-art-exhibition-major-international-forum-responsible-use-ai">published on the exhibition.</a>
  </p>

:ET